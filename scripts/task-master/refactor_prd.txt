Awesome. I’ll do a deep analysis of your semantic-reader-backend repo and compare it in detail with the docling repo (including its relation to papermage-docling) to determine:

   Which parts of your backend implementation (file-by-file) misuse or unnecessarily complicate use of docling-core and docling-parse
   Whether you can switch fully to docling itself while maintaining the same JSON output structure
   How to re-architect your pipeline system to mimic the recipe/predictor system in papermage
   Which files need to be rewritten, simplified, or removed entirely

I'll also include a complete markdown-formatted PRD with:

   A table of every file and exact instructions for each
   A high-level architecture description
   Clear, specific refactoring steps
   Pipeline/recipe redesign guidance

Give me a moment to put together the full report for you!

Refactor Plan: Aligning semantic-reader-backend with Docling
==============================================================

1\. File-by-File Analysis and Refactoring Actions
-------------------------------------------------

Below is a comprehensive table of all source files in the semantic-reader-backend repository, detailing their current roles and the plan for each in the refactored system. The plan indicates whether the file will be retained (with changes) or removed (and what will replace its functionality):

File

Current Purpose

Retain?

Refactor Changes or Replacement

analysis/document\conversion\map.py

Documents all points where conversions between Docling structures and the custom “PaperMage” JSON format occur. Essentially a developer map of format transformations.

Remove

Replace with Docling’s built-in JSON export. Docling’s unified conversion pipeline obviates the need for a manual conversion mapping log. All conversion logic will be handled by Docling’s APIs, so this mapping file becomes unnecessary.

api/base.py

Defines base classes/utilities for the API layer (could include request/response models or shared API logic).

Retain

Simplify references to pipeline. Remove any logic that orchestrates document conversion via the old pipeline. If base.py contains conversion helper functions, refactor them to call Docling’s DocumentConverter or equivalent. Ensure that response models (if any) remain consistent.

api/gateway.py

Serves as a high-level facade or gateway for document processing requests. It likely mediates between API endpoints and the pipeline/recipe, handling input files and returning JSON results.

Retain

Refactor to call Docling directly. Remove custom pipeline assembly: instead of orchestrating DoclingPdfParser and predictor classes, call Docling’s unified conversion (e.g. DocumentConverter().convert()). The gateway’s interface (function signatures) can remain, but internally it should delegate to Docling. This significantly simplifies the code – essentially a thin wrapper around Docling’s converter.

api/papermage.py

Implements the core document-processing “recipe” for the PaperMage system (possibly defines CoreRecipe or similar). Likely orchestrates the document conversion steps (parse, predictors, convert) in code, and might expose a high-level function or class for use in the API.

Retain (simplified)

Rewire to use Docling. If CoreRecipe is defined here (or in recipeapi.py), refactor it to leverage Docling’s high-level pipeline. For example, if CoreRecipe.process() currently builds a pipeline and runs predictors, change it to simply call Docling’s conversion and then format the result. If the concept of “recipe” is no longer needed with Docling (since Docling already encapsulates the recipe of steps internally), consider removing this abstraction and have the API call the converter directly.

api/recipe\api.py

Provides API endpoints for processing documents via a “recipe” (e.g. endpoints that trigger the CoreRecipe processing, possibly asynchronously). It likely wraps CoreRecipe to handle HTTP requests.

Retain (simplified)

Update endpoint implementation. Remove low-level pipeline usage and call the new high-level conversion function (from gateway or directly from Docling) that replaces CoreRecipe. The outward API (endpoints, request/response schemas) remains the same to avoid breaking clients, but internally it uses Docling’s processing. If asynchronous or background task handling is present, preserve that but apply it to the new conversion call.

api/server.py

Sets up the FastAPI (or similar) server and includes route registration. It likely ties together the API endpoints defined elsewhere.

Retain

Minimal changes. Update any imports or references if the pipeline/recipe classes move or change (for example, if server.py included the old pipeline to pre-load models, point it to Docling initialization instead). Ensure that the server still launches correctly, but now uses Docling’s models (which may initialize on first use).

api/adapters/base.py

Defines the abstract BaseAdapter class and an adapter registry for converting between formats (PDF -> Papermage JSON). In the current design, adapters encapsulate conversion logic (using parsers and converters).

Remove

Eliminate custom adapter layer. With Docling handling conversion, a separate adapter abstraction is overkill. The single Docling pipeline replaces all adapters. If any functionality from BaseAdapter (like logging or version metadata) is still needed, integrate it into the Docling call or add minimal wrapper logic. Otherwise, drop this file and deregister its use in factory.py.

api/adapters/factory.py

Registers specific adapter classes in a global registry and provides a lookup (getadapter) for converting documents. Currently used to get the PdfToPapermageAdapter.

Remove

Use Docling’s converter directly. Instead of looking up a custom adapter, call Docling’s conversion function. This registry becomes redundant because we will support essentially one primary conversion path (PDF/docx/etc. handled by Docling internally). Remove the factory and replace its usage with direct calls to Docling. For multi-format support, Docling already handles multiple input types, so a registry isn’t needed.

api/adapters/pdf.py

Implements PdfToPapermageAdapter, which uses DoclingPdfParser and the converters to produce the final JSON. This is the core glue that currently takes a PDF input and returns the custom JSON.

Remove

Replace with Docling pipeline. We will no longer manually parse PDFs or assemble JSON; instead call docling.DocumentConverter (or similar API) to get a DoclingDocument and then transform to JSON. The specific logic in convert() (handling bytes vs path, OCR flags, etc.) can be achieved by Docling’s options. For example, Docling’s convert() likely handles file paths and binary streams internally. Any flags like enableocr or detecttables will be passed to Docling’s conversion as parameters (Docling has similar capabilities – e.g. auto table detection). In short, drop this class and let Docling do what this was doing.

cli/converter\cli.py

Provides a command-line interface to convert documents using the pipeline (likely parses command-line args and invokes PdfToPapermageAdapter or CoreRecipe).

Retain (if CLI needed)

Delegate to Docling. If a CLI is still desired, simplify it to call Docling’s library or Docling’s own CLI. For example, use subprocess to call docling CLI or directly call the Python API. Alternatively, if maintaining a separate CLI command is needed (for custom output handling), have it call our new unified conversion function (which uses Docling internally). This way, all complexity is still handled by Docling. If the CLI is not actually used in deployment (perhaps superseded by API), it could be removed to avoid maintenance.

converters/docling\to\papermage\converter.py

(Very large module) Converts Docling’s internal document (DoclingDocument/PdfDocument from docling-core) into the PaperMage JSON structure. It traverses pages, text lines, tables, figures, etc., constructing the final Document (via the Pydantic models in document.py). Essentially, this is a manual JSON serialization step to match the output schema.

Remove (replace with simpler mapping)

Use Docling’s JSON or implement a lightweight mapper. Ideally, we will eliminate the need for this 3000+ line conversion logic. Docling can directly produce a “lossless JSON” export of the document ([docling · PyPI](https://pypi.org/project/docling/1.16.1/::text=Docling%20bundles%20PDF%20document%20conversion,contained%20package)) ([GitHub - docling-project/docling: Get your documents ready for gen AI](https://github.com/docling-project/docling::text=from%20docling)). We have two options: (a) Use Docling’s JSON output directly if it closely matches the current schema (less code, but verify schema differences); or (b) Write a much simpler converter that maps Docling’s unified DoclingDocument dataclass to our existing Pydantic Document model (essentially a condensed version of what this file does, leveraging the fact that Docling already parsed layout, tables, etc.). Either approach drastically reduces complexity. The preference is to let Docling export to JSON and then only tweak the format if necessary (e.g. keys or nesting differences), rather than reconstruct everything ourselves. This means this file and its intricate logic can be removed.

converters/document.py

Defines the Document data model and related classes (Entity, Span, Box) for the PaperMage JSON structure. Uses Pydantic BaseModel for easy JSON serialization. This represents the final output schema (text content, bounding boxes, entities like figures/tables, etc.).

Retain (possibly)

Reuse or update the data model. To replicate the exact JSON output, we can keep these Pydantic models as the schema. If we choose to use Docling’s JSON directly, ensure that it conforms to this schema. We may need to adjust the models slightly to accommodate any additional metadata Docling provides or to align naming. For example, Docling might output certain fields that were not in PaperMage’s original model (like more detailed table structure); we could extend Document to include them if needed or simply ignore those extras when serializing. The simplest path is to keep Document as is, and populate it from Docling’s results. That way, consumers of the API see no change in the JSON format. In summary, retain this as the canonical output schema and ensure Docling’s data is mapped into it.

parsers/docling\pdf\parser.py

Wraps doclingparse library’s DoclingPdfParserBase to obtain a PdfDocument (Docling’s internal structured representation), then invokes DoclingToPaperMageConverter to get the final JSON format. It also applies options like OCR flags and table/figure detection toggles. In short, this class currently performs: Parse PDF -> produce DoclingDocument -> convert to JSON dict.

Remove

Use Docling’s unified parsing. Instead of manually invoking doclingparse and then converting, call Docling’s high-level API. Docling’s DocumentConverter likely encapsulates parsing PDF to a DoclingDocument and may even allow direct JSON export. Thus, we don’t need our own DoclingPdfParser class. The functionality is replaced by Docling itself. We will remove this class and update all references (adapters, pipeline, etc.) to call Docling’s converter. Any special handling (like storing interim PdfDocument or tweaking parser params) should be done via Docling’s conversion options. For example, where we previously did parser = DoclingPdfParserBase(kwargs) and then parser.parse(), we will do doc = DocumentConverter(options).convert(path) using Docling’s parameters (Docling likely auto-detects and processes tables, figures if not disabled).

pipeline/pipeline.py

Implements a custom Pipeline framework: defines DocumentProcessor (abstract class for processing a DoclingDocument), PipelineStep (wrapper for a processor + execution conditions), and Pipeline (which holds a sequence of steps to execute in order). This was a general mechanism to run the document through multiple processing stages (layout parsing, table detection, figure detection, etc.), including error handling and conditional execution.

Remove

Drop custom pipeline architecture. Docling’s design already sequences parsing and enrichment internally, so we do not need to manage a pipeline ourselves. Instead of Pipeline and PipelineStep, we will rely on Docling to perform all necessary steps in the correct order. Therefore, we can remove this entire framework. Any error handling or conditional logic needed can be implemented around the single conversion call if required (for example, catching exceptions from Docling conversion and returning a formatted error). By removing this, we eliminate a large source of complexity (the need to implement retry logic and step-by-step stats ourselves).

pipeline/simple\pipeline.py

Provides a higher-level SimplePipeline that likely extends Pipeline with convenience methods to add common processors and to convert to/from the PaperMage format. Notably, it uses PaperMageAdapter to convert external docs to DoclingDocument and back. It may define a PredictorProcessor to wrap predictor classes into the pipeline. Essentially, it’s an easier interface to build the pipeline defined in pipeline.py.

Remove

No longer needed. With the removal of the custom Pipeline, the SimplePipeline abstraction has no purpose. We won’t be assembling pipelines by adding processors; instead we call Docling. Thus, this file and its functions (like frompapermage and topapermage using PaperMageAdapter) become obsolete. Remove it, and remove references to it (e.g. if any code was calling SimplePipeline.addprocessor or SimplePipeline.processfile). The new approach doesn’t require these manual steps at all.

predictors/figure\predictor.py

Contains FigurePredictor, a class that uses Docling’s figure detection model to identify figures in the document and extract images/captions. It likely calls some docling-core or external model (maybe via docling’s API or HuggingFace) to find image elements and then adds them to the DoclingDocument. This class is used as a step in the pipeline to augment the parsed document with figure data.

Remove

Leverage Docling’s figure detection. Docling natively handles figure extraction as part of its “advanced PDF understanding” ([GitHub - docling-project/docling: Get your documents ready for gen AI](https://github.com/docling-project/docling::text=,gapped%20environments)). Therefore, we don’t need a separate predictor class. If Docling’s default pipeline includes figure detection, it will populate the output with figures and their metadata. In case Docling’s current version requires explicit enabling of figure analysis, we will enable it via Docling’s conversion options (or possibly use Docling’s “Figure enrichment” plugin ([GitHub - docling-project/docling: Get your documents ready for gen AI](https://github.com/docling-project/docling::text=,Markdown%2C%20HTML%2C%20and%20lossless%20JSON)) ([GitHub - docling-project/docling: Get your documents ready for gen AI](https://github.com/docling-project/docling::text=PDF%2C%20DOCX%2C%20XLSX%2C%20HTML%2C%20images%2C,Unified%2C%20expressive%20DoclingDocument)) if provided). The net effect is that figure detection happens within Docling, and this standalone class can be removed. All code that instantiates or configures FigurePredictor (e.g. in recipes or pipeline) will be deleted.

predictors/language\predictor.py

Contains LanguagePredictor, which detects the primary language of the document (and possibly if multiple languages or RTL text are present). It may use text samples or docling-core metadata to determine language codes and mark DoclingDocument.metadata.language.

Remove

Use Docling’s metadata extraction. Docling’s feature set includes language detection for documents ([docling · PyPI](https://pypi.org/project/docling/1.16.1/::text=,OCR%20support%20for%20scanned%20PDFs)). We will rely on Docling to set the document’s language metadata (Docling likely uses heuristics or a language detection library under the hood). Any RTL (right-to-left) detection, which rtlutils.py aided, is also likely handled by Docling or can be done with far less custom code. We can drop this predictor. After Docling conversion, if needed, we can read the language from Docling’s output and include it in the JSON (ensuring it appears in the same place as before, e.g. in metadata). If Docling’s language detection differs slightly in output format (e.g. different language codes), we can adapt it to match the previous output (e.g. map ISO codes to names if needed). No separate predictor class is necessary.

predictors/rtl\utils.py

Utilities for detecting if a language is RTL (right-to-left) and possibly for reversing text or handling bidi issues. Likely used by LanguagePredictor to mark isrtllanguage flags in metadata or adjust text direction.

Remove

Integrate minimal RTL logic if needed. If Docling’s language detection already provides an RTL flag or handles script direction, we can remove rtlutils entirely. If not, and the output JSON previously included something like "isrtl": true/false, we can derive that in one line from the language code (e.g. mark true if language code is ar, he, etc.). This lightweight check can be done inline when assembling output, rather than a whole utility module. Thus, a separate module is not needed. Remove it along with LanguagePredictor.

predictors/structure\predictor.py

Contains StructurePredictor, which likely handles higher-level document structure detection (sections, headings, list of references, etc.) using docling-core algorithms. It might call into docling-core’s layout analysis or external models to identify sections, or simply rely on the content parsed to group lines into paragraphs/sections. In the pipeline, this would run after basic parsing to annotate the document with structural info.

Remove

Rely on Docling’s layout and reading order. Docling inherently produces a structured document with reading order (and possibly some segmentation into sections). If the current StructurePredictor was doing something beyond Docling’s capabilities (for example, identifying section titles or reference sections), we should verify if Docling now covers it. Docling’s roadmap includes metadata extraction like title and references, but if not fully available, we may temporarily drop this feature or handle it differently. Given the complexity and the goal to simplify, we remove this predictor. Any essential structural info that the Semantic Reader needs (e.g. the reading order of text) is provided by Docling’s core conversion. If section detection is critical and Docling doesn’t do it yet, consider a follow-up step using a simpler approach or incorporate a Docling plugin when available. For this refactor, the focus is on eliminating custom code, so we accept the Docling output as-is for structure.

predictors/table\predictor.py

Contains TablePredictor, which uses Docling’s table analysis model (TableFormer) to detect and structure tables. It likely takes the raw table text from the parser and produces structured rows and cells, inserting them into the DoclingDocument. This is one of the most complex predictors (table structure is complicated and the file is 3800 lines).

Remove

Use Docling’s table structure recovery. One of Docling’s key features is recovering table structures from PDFs ([docling · PyPI](https://pypi.org/project/docling/1.16.1/::text=,order%20and%20recovers%20table%20structures)), presumably using TableFormer or a similar model under the hood. We will rely on Docling to identify tables and output them in the JSON. This means eliminating the entire custom TablePredictor class and the integration of the TableFormer model that it handled. Docling likely requires the TableFormer model weights (Docling might automatically download or prompt for them); ensure those are available (perhaps via the docling-ibm-models package or huggingface). Once configured, Docling will populate the document with table structures. Our output JSON should reflect those (and our Document Pydantic model may need to accommodate structured table data if it didn’t before – e.g. ensure that tables are represented similarly to how TablePredictor did). In summary, remove this class and trust Docling’s implementation for table detection and cell segmentation.

rasterizers/pdf\rasterizer.py

Provides functionality to rasterize PDF pages to images (perhaps using Poppler or PIL). This was probably used by the figure or table predictors which needed image data (for example, to run image-based models like figure classification or table structure detection on table images).

Remove

No longer needed explicitly. If Docling’s pipeline requires page images (for visual models), Docling will handle that internally. For example, TableFormer might need an image of each table – Docling’s integration likely takes care of extracting table region images behind the scenes ([Docling Technical Report - arXiv](https://arxiv.org/html/2408.09869v2::text=Docling%20Technical%20Report%20,crop%20of%20the%20table)). Similarly, figure detection in Docling could either use PDF’s embedded images or rasterize if needed. We do not need to manually rasterize anything. Therefore, remove this utility. If in some rare case we still need to get an image (e.g. to return a figure thumbnail URL or so), we would generate it on-the-fly using a simpler approach or use Docling’s extracted images. But currently, the JSON output probably included image data in base64 or paths – Docling can provide images for figures (perhaps encoded or saved). So the rasterizer module can be dropped.

Note: The app layer of the project (FastAPI routes, services, etc.) will also be adjusted to use the refactored internals. For instance, app/pipelines.py – which currently does:

    from papermagedocling import CoreRecipe, DoclingPdfParser
     ... then uses CoreRecipe or DoclingPdfParser to process files ...
    

will be simplified. We will remove references to CoreRecipe/DoclingPdfParser and instead call our new conversion gateway (which in turn calls Docling). The app code does not require a full breakdown per file here since its structure remains the same; only its calls into the papermagedocling module change to the new interface.

2\. Docling vs. Current System: Conceptual Mismatches
-----------------------------------------------------

The current implementation in semantic-reader-backend diverges from Docling’s intended usage in several ways. Here’s an overview of the key conceptual mismatches and how we plan to resolve them:

   Pipeline Orchestration vs. Built-in Conversion: The current system manually orchestrates a multi-step pipeline (parse -> figure detection -> table detection -> etc.) using custom classes (PipelineStep, PredictorProcessor, etc.). Docling, on the other hand, is designed to handle these steps internally. Docling expects the developer to simply call a high-level conversion function (or use its CLI) and let Docling decide the sequence of parsing and enrichment. The mismatch: semantic-reader-backend tried to micromanage the pipeline (even recreating an abstract pipeline framework), whereas Docling provides a ready-made pipeline. We will shift to Docling’s pipeline – meaning no external pipeline management is needed. The refactored system will trust Docling’s DocumentConverter (or similar) to execute all necessary sub-tasks in the correct order.
    
   Predictor Classes vs. Docling Enrichment Plugins: In Docling’s architecture, after the initial parse of a document, various enrichment modules (like table structure recovery, figure extraction, formula detection, etc.) can be applied, often automatically if the necessary models are available ([GitHub - docling-project/docling: Get your documents ready for gen AI](https://github.com/docling-project/docling::text=,gapped%20environments)). These are not typically invoked by separate user-defined classes; instead, Docling either runs them by default or via simple flags. The current backend, however, implements each enrichment as a distinct Predictor class that must be invoked sequentially. For example, TablePredictor in the current system explicitly loads a model and processes the document’s tables, whereas in Docling, table detection is an integrated feature – no need for a user to manually instantiate a “TablePredictor.” This mismatch means our current design is more complex and tightly coupled to model details. By refactoring, we align with Docling’s model: we will remove predictor classes and let Docling handle those tasks. Essentially, we reduce the system’s responsibility to “ask Docling to enrich the doc with tables/figures” instead of performing it ourselves.
    
   Docling-Core Data Types vs. Unified Docling Document: Previously, the backend used docling-core types like DoclingDocument, PdfPage, TextCellUnit etc., and then immediately converted them to a custom JSON. This two-step representation (Docling’s internal objects -> custom objects) is redundant. Docling’s intended design is that you use the DoclingDocument (a unified, Pydantic data model for the document ([GitHub - docling-project/docling: Get your documents ready for gen AI](https://github.com/docling-project/docling::text=,Various%20export%20formats%20and%20options))) directly or its serialization. The conceptual mismatch was that semantic-reader-backend did not directly use DoclingDocument’s capabilities; instead it transformed it into a parallel structure (PaperMage Document). In the refactor, we will embrace DoclingDocument. We will either use DoclingDocument’s modeldump() / JSON export directly, or map it to our output model if minor adjustments are needed. The key is we won’t treat DoclingDocument as a “foreign” object that must be immediately walked and copied; instead, it becomes the central representation of the parsed document in memory. This reduces duplication of schemas and ensures we’re aligned with Docling’s data model (which is well-maintained and evolving).
    
   “Recipe” Abstraction vs. Direct Conversion Call: The current code introduces a concept of “Papermage recipes” (e.g. CoreRecipe), likely to configure which predictors run (like a recipe might say: parse PDF, then run these predictors, then convert). This is an additional abstraction layer on top of the pipeline. Docling does not require a recipe specification by the user for core functionality; you might only configure Docling via simple parameters or by enabling/disabling certain plugins. The mismatch is that our system’s recipe mechanism adds complexity (configuring pipeline steps and instantiating classes) that Docling’s design doesn’t call for. In the refactored system, this recipe layer will be removed or simplified to a thin configuration for Docling. For example, instead of constructing a CoreRecipe object with various predictor flags, we will call DocumentConverter(converttables=True, convertfigures=True, ...) or pass equivalent flags. The end goal is one function call to handle a given “recipe” rather than an orchestrated sequence. This dramatically simplifies usage: the API endpoint can simply call convertdocument(file, options) and get results, rather than building a recipe and running it.
    
   Output Schema Ownership: Docling defines a rich JSON schema for documents (as seen in docling-core’s README ([docling-core/README.md at main - GitHub](https://github.com/docling-project/docling-core/blob/main/README.md::text=docling,attached%20to%20the%20converted))). The current system defines its own JSON schema (via converters/document.py) to serve Semantic Reader’s needs. There’s a partial overlap (both describe pages, lines, figures, tables, etc.), but names and organization might differ. The mismatch is that we haven’t been using Docling’s JSON schema as-is, possibly because the Semantic Reader required a specific format. Post-refactor, while we will continue to output the same JSON structure as before (to avoid breaking clients), we will achieve it with much less effort by leveraging Docling’s data. In the long run, as Semantic Reader and Docling evolve, we might consider converging on a single schema. For now, however, we treat Docling’s output as the source and transform it minimally to match the expected output. This means our system remains a thin adapter layer on top of Docling (which is a valid use of Docling), rather than re-implementing parsing or prediction.
    

In summary, the refactor will realign the backend with Docling’s philosophy: instead of managing low-level details (parsing, table detection, etc.) ourselves, we delegate to Docling’s higher-level API. This eliminates the conceptual duplication (our pipeline vs Docling’s pipeline, our predictors vs Docling’s models) and resolves the mismatches listed above.

3\. Comparison of Pipelines and Workflow (Docling vs. Current)
--------------------------------------------------------------

To illustrate the changes, here is a deep-dive comparison of how a PDF document is processed in the current system versus how it will be processed using Docling. We break down the stages and components involved:

   Current semantic-reader-backend Processing (“Papermage” Recipe & Pipeline):
    
    1.  PDF Input Ingestion: A PDF file path or bytes enters the system via an API call or CLI. The system chooses a recipe (e.g. CoreRecipe) which defines what to do.
    2.  Parsing (Docling Parse): The recipe calls DoclingPdfParser.parse() (from docling-parse) to extract raw content. This yields a PdfDocument object (containing text elements with coordinates, images, etc. from the PDF) ([semantic-reader-backend/src/papermage\docling/api/adapters/pdf.py at master · DavidKric/semantic-reader-backend · GitHub](https://github.com/DavidKric/semantic-reader-backend/blob/master/src/papermagedocling/api/adapters/pdf.py::text=def%20convert%28self%2C%20document%3A%20Any%2C%20,str%2C%20Any)) ([semantic-reader-backend/src/papermage\docling/api/adapters/pdf.py at master · DavidKric/semantic-reader-backend · GitHub](https://github.com/DavidKric/semantic-reader-backend/blob/master/src/papermagedocling/api/adapters/pdf.py::text=,from%20kwargs)).
    3.  DoclingDocument Creation: The parser wraps the parsed content into a DoclingDocument (from docling-core.types), which is an in-memory representation of the doc. At this stage, it has basic layout (pages, lines, detected text blocks) but limited higher-level semantics.
    4.  Predictor Sequence (Pipeline): The CoreRecipe then runs a series of predictors on the DoclingDocument, typically via the pipeline:
           Figure Predictor: Scans the DoclingDocument for images or drawing objects, uses ML models to classify or extract figures, and appends figure metadata (e.g. bounding boxes, captions) ([semantic-reader-backend/src/papermage\docling/predictors/figure\predictor.py at master · DavidKric/semantic-reader-backend · GitHub](https://github.com/DavidKric/semantic-reader-backend/blob/master/src/papermagedocling/predictors/figurepredictor.py::text=class%20FigurePredictor%3A)).
           Table Predictor: Locates tabular structures (often by analyzing layout or grid lines) and applies the TableFormer model to structure cells into rows/columns, updating the document’s table data ([semantic-reader-backend/src/papermage\docling/predictors/table\predictor.py at master · DavidKric/semantic-reader-backend · GitHub](https://github.com/DavidKric/semantic-reader-backend/blob/master/src/papermagedocling/predictors/tablepredictor.py::text=class%20TablePredictor%3A)).
           Language Predictor: Analyzes text to determine language and sets doc.metadata.language (and potentially marks if RTL) ([semantic-reader-backend/src/papermage\docling/predictors/language\predictor.py at master · DavidKric/semantic-reader-backend · GitHub](https://github.com/DavidKric/semantic-reader-backend/blob/master/src/papermagedocling/predictors/languagepredictor.py::text=class%20LanguagePredictor%3A)).
           Structure Predictor: (If used) Identifies sections like headings or references (e.g. by bold font or keywords) and annotates the document.
           (Each predictor is implemented in its respective class and possibly uses external libraries or models. The pipeline runs them in order, each modifying the shared DoclingDocument.)
    5.  Conversion to JSON: After all predictors, the enriched DoclingDocument (now containing figures, tables, etc.) is passed to DoclingToPaperMageConverter. This conversion traverses every page, line, span, table, figure, etc., constructing a nested JSON (or Pydantic Document) that matches the Semantic Reader’s expected output format ([semantic-reader-backend/src/papermage\docling/converters/docling\to\papermage\converter.py at master · DavidKric/semantic-reader-backend · GitHub](https://github.com/DavidKric/semantic-reader-backend/blob/master/src/papermagedocling/converters/doclingtopapermageconverter.py::text=%40staticmethod)) ([semantic-reader-backend/src/papermage\docling/converters/docling\to\papermage\converter.py at master · DavidKric/semantic-reader-backend · GitHub](https://github.com/DavidKric/semantic-reader-backend/blob/master/src/papermagedocling/converters/doclingtopapermageconverter.py::text=)). The output includes:
           Full text content (with fulltext or similar),
           A list of pages, each with size and a list of words/tokens and higher-level entities (like figures, tables, equations) identified,
           Structured objects for tables (with rows and cells text),
           Metadata like language and flags (e.g. isrtllanguage).
    6.  Result Delivery: The resulting JSON (or Document model instance) is returned by the recipe and then by the API endpoint to the client.
    
    Throughout this process, a lot of internal state is managed explicitly: e.g. the pipeline collects stats on each step, error handling for each predictor is done stepwise (via PipelineStep’s retry logic) ([semantic-reader-backend/src/papermage\docling/pipeline/pipeline.py at master · DavidKric/semantic-reader-backend · GitHub](https://github.com/DavidKric/semantic-reader-backend/blob/master/src/papermagedocling/pipeline/pipeline.py::text=class%20PipelineStep%3A)) ([semantic-reader-backend/src/papermage\docling/pipeline/pipeline.py at master · DavidKric/semantic-reader-backend · GitHub](https://github.com/DavidKric/semantic-reader-backend/blob/master/src/papermagedocling/pipeline/pipeline.py::text=condition%3A%20Optional%20condition%20function%20to,if%20this%20step%20should%20execute)), and intermediate data (like PdfDocument) is converted by custom code. The “Papermage recipe” essentially hardcodes the pipeline of Docling parse + specific predictors + conversion.
    
   Refactored Processing with Docling (Native Pipeline/Models):
    
    1.  PDF Input Ingestion: (Unchanged) The PDF file or bytes is received via API or CLI.
        
    2.  Docling Conversion Call: We call Docling’s high-level DocumentConverter (or an equivalent docling.convert() function) with the file and desired options. For example:
        
            from docling.documentconverter import DocumentConverter
            converter = DocumentConverter(tables=True, figures=True, metadata=True)  
            result = converter.convert(sourcefile)
            
        
        This single call replaces the manual sequence of parse + predictors. Internally, Docling will:
        
           Parse the PDF into its unified DoclingDocument (using docling-parse under the hood).
           Enrich with Tables: If tables are enabled (which we request via tables=True or by default), Docling feeds detected table regions through its Table structure model (TableFormer) automatically ([Docling Technical Report - arXiv](https://arxiv.org/html/2408.09869v2::text=Docling%20Technical%20Report%20,crop%20of%20the%20table)) ([Docling Technical Report - arXiv](https://arxiv.org/html/2408.09869v2::text=The%20Docling%20pipeline%20feeds%20all,crop%20of%20the%20table)). The DoclingDocument’s table objects are populated with cells and content.
           Enrich with Figures: Docling will identify images/graphics in the document. With figure enrichment on, it could classify or caption them as needed. (Docling can use visual language models like SmolDocling for image classification if configured, though for our purposes basic figure extraction is sufficient.) The DoclingDocument now has Figure entities with images or references.
           Language & Metadata: Docling extracts metadata such as document language, title, etc. as part of the conversion if available ([docling · PyPI](https://pypi.org/project/docling/1.16.1/::text=,OCR%20support%20for%20scanned%20PDFs)). The DoclingDocument’s metadata fields will be set (e.g. language = "en" and maybe isrtl = False).
           Reading Order and Structure: Docling ensures that the text is in logical reading order and might chunk paragraphs. (If needed, Docling’s output can include a hierarchical structure, though we may or may not use it directly.)
        
        Importantly, all of this happens inside the convert() call, so from our perspective we have one DoclingDocument result that already contains what was previously added by multiple predictors.
        
    3.  Docling Document to JSON: We obtain result.document, which is Docling’s DoclingDocument object (likely a Pydantic model itself). We then need to produce the JSON output in our specific format:
        
           EITHER use result.document.modeldump() (or exporttojson() if provided) to get Docling’s JSON, and then post-process keys to match the legacy format.
           OR manually map result.document to our Document Pydantic model. For instance, iterate over result.document.pages and for each:
               Create a Page entry in our Document.pages list with width, height from Docling.
               Map Docling’s lines or tokens into our words list (ensuring positions and text match).
               For each figure in Docling’s output, create a Figure entity in our JSON with the same coordinates and caption.
               For each table in Docling’s output, create a Table entity in our JSON, preserving structure.
               Copy metadata like language.
        
        Because DoclingDocument is quite similar in concept to our output, this mapping is straightforward – far simpler than the original converter which had to assemble everything from scratch. For example, Docling might already provide text spans with bounding boxes, so we just need to format them into our Span model ([semantic-reader-backend/src/papermage\docling/converters/docling\to\papermage\converter.py at master · DavidKric/semantic-reader-backend · GitHub](https://github.com/DavidKric/semantic-reader-backend/blob/master/src/papermagedocling/converters/doclingtopapermageconverter.py::text=%23%20Process%20textline%20cells%20,rows)) ([semantic-reader-backend/src/papermage\docling/converters/docling\to\papermage\converter.py at master · DavidKric/semantic-reader-backend · GitHub](https://github.com/DavidKric/semantic-reader-backend/blob/master/src/papermagedocling/converters/doclingtopapermageconverter.py::text=)).
        
        Essentially, Docling’s JSON and our JSON differ mainly in naming/organization, not in fundamental data. We act as a translator between the two schemas at the final step.
        
    4.  Result Delivery: The final JSON (which matches the previous structure exactly) is returned to the client. The client should notice no difference in content or fields compared to the pre-refactor output, aside from potential improvements (Docling may extract more accurate tables or include additional metadata, which we can include or omit based on requirements).
        

Diagram of Refactored Flow: (Pseudo-code for clarity)

     Before (multiple steps):
    pdf = open(filepath, 'rb')
    pdfdoc = DoclingPdfParser().parse(pdf)
    figures = FigurePredictor().process(pdfdoc)
    tables = TablePredictor().process(pdfdoc)
    language = LanguagePredictor().process(pdfdoc)
    documentjson = DoclingToPaperMageConverter.convertpdfdocument(pdfdoc)
    return documentjson
    
     After (single integrated step):
    result = DocumentConverter(tables=True, figures=True).convert(filepath)
    doc = result.document   This is DoclingDocument
    documentjson = mapdoclingtopapermage(doc)   lightweight formatting
    return documentjson
    

As shown above, the new flow collapses what used to be 5 distinct processing stages into essentially one stage (Docling conversion) followed by a formatting step. This not only reduces code but also leverages the robust, tested pipeline inside Docling, which is less error-prone and easier to maintain (Docling’s updates will automatically improve our backend’s capabilities).

4\. Step-by-Step Technical Refactor Plan
----------------------------------------

Refactoring the system to use Docling with minimal disruption will be done in iterative steps. This plan ensures that at each step, the system remains functional (tests can be run to catch regressions) and the external behavior (API and output) remains consistent:

Step 1: Introduce Docling Library and Update Dependencies

   Add docling to the project dependencies (in requirements or pyproject). Remove docling-core and docling-parse packages from dependencies. Docling v2+ already encapsulates core and parse functionality ([docling · PyPI](https://pypi.org/project/docling/1.16.1/::text=Docling%20bundles%20PDF%20document%20conversion,contained%20package)).
   Ensure that any required model packages (like docling-ibm-models for TableFormer or SmolDocling if needed) are available. This might involve installing extra dependencies or letting Docling download models at runtime. (Docling’s documentation suggests it will handle model downloads or provide instructions.)
   Verify installation by writing a small test snippet (not in production code) to parse a sample PDF with Docling. This will confirm that Docling can run in our environment and that torch and other heavy dependencies are resolved.

Step 2: Create a New Unified Conversion Function

   Implement a new function (for example, convertdocument(filebytes, options)) in a suitable place (perhaps in papermagedocling.gateway or a new papermagedocling.converter module). This function will:
       Accept an input file (path or bytes) and conversion options (e.g. whether to do OCR, etc.).
       Internally instantiate DocumentConverter (from Docling) with appropriate flags. For initial parity with current features, enable table detection, figure detection, and metadata extraction. For example:
        
            converter = DocumentConverter(detecttables=True, detectfigures=True, ocrlang=..., enableocr=...)
            
        
        Use enableocr only if the original system had OCR functionality toggled (the current code has enableocr flag in PdfAdapter ([semantic-reader-backend/src/papermage\docling/api/adapters/pdf.py at master · DavidKric/semantic-reader-backend · GitHub](https://github.com/DavidKric/semantic-reader-backend/blob/master/src/papermagedocling/api/adapters/pdf.py::text=,from%20kwargs))).
       Call converter.convert(source).
       Obtain the resulting DoclingDocument (result.document).
       Map the DoclingDocument to the PaperMage JSON format. Initially, you can use Docling’s own JSON export and compare it to the expected output to identify differences. Then adjust: e.g., if Docling’s JSON has a list of blocks but our output expects words list, iterate and build that.
       Return the final JSON dictionary (or Pydantic model).
   Do not remove the old code yet in this step. Instead, implement this alongside. We can add toggling logic (for instance, an environment variable or config flag) to switch between the old pipeline and new Docling path for testing. This way, we can run tests on both paths to ensure parity.

Step 3: Integrate the New Conversion Function into the API

   Modify api/gateway.py and/or api/recipeapi.py to use the new conversion function instead of the old pipeline. For example, in the endpoint handler for document conversion, replace:
    
        recipe = CoreRecipe(options)
        output = recipe.process(file)
        
    
    with:
    
        output = convertdocument(file, options)
        
    
   At this point, if we enable the new path, the API should start returning Docling-produced results. Run the test suite (especially any integration tests comparing JSON output). The output should be logically the same. If any keys differ or some expected field is missing, adjust the mapping in convertdocument. For instance, ensure the JSON has "fulltext" if clients expect it (Docling might name it differently or require concatenation of all text).
   Iterate on this until tests pass. This step is critical to verify that switching to Docling doesn’t break external contracts. Because we still have the old code available, we can cross-validate: run a sample through both pipelines (old and new) and diff the JSON. They should match. If not, make targeted fixes:
       Add any missing metadata (Docling might not automatically provide some data that the old code did; e.g., if the old pipeline added a custom {"hasequations": false} flag and Docling has no notion of it, we might set it to false for continuity, or drop it if it’s not used by clients).
       Remove or translate extra fields if Docling’s JSON is richer. We might choose to drop information that the front-end doesn’t expect to see (to strictly replicate output). Alternatively, extra info can be kept as it likely won’t break clients that ignore unknown fields, but the PRD suggests minimizing change, so lean towards replicating exactly.

Step 4: Remove and Refactor Redundant Components

Once the new Docling-based conversion is producing correct output, we can safely remove the now-unused code. This step can be done in stages or all at once:

   Delete predictor classes and their tests. This includes figurepredictor.py, tablepredictor.py, languagepredictor.py, structurepredictor.py, and rtlutils.py. Before deletion, ensure nothing is importing them. Our new converter should not be using them at all. Remove any leftover import statements or references (for example, SimplePipeline might have created PredictorProcessor(FigurePredictor), which we’ve stopped using).
   Remove the entire pipeline framework: pipeline.py and simplepipeline.py. Again, confirm that nothing calls Pipeline or SimplePipeline anymore. Our gateway should bypass this completely now.
   Remove DoclingPdfParser and converters: doclingpdfparser.py and doclingtopapermageconverter.py can be eliminated. The new conversion function replaces their functionality. Also remove the parsers package if it becomes empty and the converters package if only document.py remains. (We will keep document.py for the Pydantic models, unless we decide to embed those models in another module.)
   Simplify the adapters: Delete api/adapters/pdf.py, api/adapters/base.py, and api/adapters/factory.py. These are tightly tied to the old parsing and converting logic. After our changes, nothing should call PdfToPapermageAdapter or the adapter registry. If PaperMageAdapter was referenced in simplepipeline or elsewhere, remove those references.
   Update imports: With many files gone, remove or update import statements throughout the codebase:
       In api/papermage.py or api/recipeapi.py, if they imported CoreRecipe from a now-removed module, adjust them. Possibly, we will remove CoreRecipe entirely. If the API functions were directly constructing CoreRecipe, they now simply call convertdocument. Thus, no import needed except our converter function.
       In app/pipelines.py (if that file is kept for asynchronous tasks), replace from papermagedocling import CoreRecipe, DoclingPdfParser with from papermagedocling import convertdocument. Also simplify its logic accordingly.
       Remove any doclingcore.types and doclingparse imports (they existed in pipeline and parser classes). We rely solely on import docling.
   Migrate any essential logic from removed code that Docling doesn’t cover:
       If StructurePredictor was adding something like marking section headings in the JSON and Docling doesn’t do that yet, decide if this feature is critical. If yes, implement a small post-processing in our converter: e.g., detect if a line is all bold uppercase and tag it as a heading in the JSON. This keeps the feature without bringing back the whole predictor. If not critical, defer it – possibly document that section heading tagging is temporarily disabled until Docling supports it.
       If rtlutils provided an isrtllanguage boolean and Docling only gives language code, add a line in our converter:
        
            document.isrtl = (doc.language in ["ar", "he", "fa", "ur"])
            
        
        to maintain that field in output.
       If any database or caching in pipeline (for example, caching model results) was present, consider if needed. Probably not, since Docling will manage performance (and we can rely on its internal caching if any).
   Run tests again after removal to ensure nothing is broken by the deletions. Our toggling mechanism can now be removed or permanently set to use Docling (since the old code is gone).

Step 5: Optimize and Clean Up

   With the new system in place, evaluate if we can further simplify:
       The document.py models: Are they all still needed? Perhaps some were only used internally by the converter (like Span or Box). If our output uses them, keep. If not (e.g., if previously Span was an internal construct for converter and final JSON inlines spans differently), we might remove unused ones. Since we aim not to change output, likely we keep them.
       Logging and error handling: The old pipeline had per-step error handlers and retry logic. Now, if something fails inside Docling’s conversion, an exception will be thrown. We should handle this at a high level – e.g., wrap converter.convert() in a try/except in convertdocument and log an error or raise a controlled exception (maybe returning a JSON error response). Docling is stable, but OCR or model loading errors could occur. Our new error handling strategy can be much simpler (no per-step retry – just fail the whole conversion if something goes wrong, possibly return a 500 with an error message). This aligns with typical API behavior and is acceptable unless specific retry logic is needed for flaky OCR. Given minimal disruption, we will ensure the API error format remains the same as before. If previously the pipeline caught errors and returned a JSON with an "error" field, we implement similar wrapping around Docling.
       Performance considerations: Remove any now-unused heavy objects from memory promptly. For example, if Docling returns a huge object and we only need the JSON, ensure we don’t keep the DoclingDocument around unnecessarily. Possibly call del doc after conversion or stream the output. This is minor but could help memory usage in large batch conversions.
       Remove any references to “papermage” terminology if not needed. Our code can become more straightforward (e.g., we might rename convertdocument to something like convertwithdocling internally, though externally it doesn’t matter). The idea is to make the code self-documenting that Docling is now the engine.

Step 6: Schema and Output Validation

   As a final verification, we will test the output JSON structure against known examples from the old system. For instance, pick a sample PDF with a table and a figure. Run the old version (if possible, from a tag or branch) to get JSON, and run the refactored version to get JSON. Do a thorough comparison:
       Ensure all top-level keys (e.g., pages, entities, fulltext, metadata) match exactly.
       Ensure lists of words/tokens are in the same order and have the same text content.
       Table structures: verify that the number of rows and columns and the text in each cell is identical. (Docling’s table detection might actually improve the structure – that’s fine as long as it’s not worse. Improved table structure is a net benefit and should be backward-compatible in format.)
       Figure data: check that each figure in the old output has a corresponding figure in the new output with the same caption and page location. If Docling finds an extra figure that was previously missed, that’s actually a positive change (we can include it – clients likely will handle an extra figure in the JSON gracefully, since it’s additional data of the same schema).
       Language: confirm that languagename or similar fields are consistent (e.g., if previously we output "language": "English", ensure we still do, or at least "language": "en" consistently if that was expected). We might need to map ISO codes to full names if the old format did so (the snippet in rtlutils suggests they stored languagename = "English" and code separately ([semantic-reader-backend/src/papermage\docling at master · DavidKric/semantic-reader-backend · GitHub](https://github.com/DavidKric/semantic-reader-backend/tree/master/src/papermagedocling::text=languagename%20%3D%20pdfdoc,get%28%27isrtllanguage%27%2C%20False))).
   Any discrepancies will be addressed either by adjusting the mapping or, if trivial differences, by communicating them. But since the goal is no changes visible, we attempt to iron them all out.
   Finally, update documentation (the README in app/README.md shown in the repo) to reflect that our pipeline is now powered by Docling. This is more of a housekeeping step: e.g., mention that we use Docling for PDF conversion, and remove references to the now-deleted components. This helps future maintainers understand the new design.

Step 7: Package and Deploy

   Remove any leftover dead code or files (if any parts of papermagedocling directory are entirely empty after the purge, delete the folder).
   Run the full test suite and do manual testing via the API (upload a PDF via the API endpoint, check the response).
   Because this is a major internal refactor, perform a careful code review and perhaps a load test on a staging environment. We expect memory and CPU usage to be somewhat affected (Docling might use GPU if available for faster processing, which is a bonus). Ensure that timeouts or large file handling are acceptable.
   Once satisfied, proceed to deploy the refactored version. Minimal disruption to users is achieved since the external API and JSON format remain consistent. The improved internals should be transparent, except possibly faster processing and better extraction quality.

Note on Schema Changes: We have taken care to avoid any schema changes to the output. The JSON structure and keys should remain identical for all existing fields. If Docling provides extra fields (for example, maybe DoclingDocument has an id for each element), we will exclude them unless we have a reason to expose them. This conservative approach ensures clients do not break. Over time, we might introduce new fields (since Docling can yield richer data, e.g. an ocrconfidence or tablehtml etc.), but that would be done in a controlled, backward-compatible manner (additive fields with documentation). In this immediate refactor, the output JSON is a drop-in replacement for the old output.

5\. Conclusion and Developer Notes
----------------------------------

By following this plan, we dramatically reduce complexity in the codebase and align it with Docling’s architecture. The new implementation will be easier to maintain (upgrading Docling can automatically improve the backend’s capabilities) and leaner (many thousands of lines of custom code removed).

This PRD outlined the necessary changes file-by-file, highlighted conceptual differences (pipeline vs. converter, predictors vs. built-in models), and provided a roadmap for implementation. The end result is a Semantic Reader Backend that uses Docling as the engine to convert documents to JSON, with only a thin layer on top to conform to the expected output format ([docling · PyPI](https://pypi.org/project/docling/1.16.1/::text=Docling%20bundles%20PDF%20document%20conversion,contained%20package)). Developers can then focus on further features (if needed) without worrying about maintaining custom parsing logic, and trust Docling’s robust pipeline for the heavy lifting.